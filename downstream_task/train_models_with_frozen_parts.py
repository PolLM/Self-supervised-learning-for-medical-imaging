# -*- coding: utf-8 -*-
"""Model freezing parts.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e7zGFneaU6MaQL9g52BblK2gZqnBDDq_
"""

import os

import torch

import torch.nn as nn
from torchvision.transforms import transforms
from torchvision import transforms, datasets

import numpy as np
from torchvision.transforms import transforms

from PIL import Image, ImageOps, ImageFilter
import torchvision.transforms as transforms



from torchvision import models
#import torchvision.transforms as transforms
import torchvision.datasets as dsets

import matplotlib.pylab as plt

from torch.utils.data import DataLoader
import torchvision.models as models
import numpy as np

import torchvision
from torchvision import transforms
from torchvision.datasets import ImageFolder


from torch.utils.data import DataLoader
import torchvision.models as models

from torchvision.datasets import ImageFolder
from torch.utils.tensorboard import SummaryWriter
import math
from torchvision import transforms


def accuracy_batch(labels, outputs):
    total_acc = []
    for lab, out in zip(labels, outputs):
        lab, out = torch.tensor(lab), torch.tensor(out)
        preds = out.argmax(-1)
        acc = (preds == lab.view_as(preds)).float().detach().numpy().mean()
        total_acc.append(acc)
    return np.mean(total_acc)

def accuracy(labels, outputs_logits):
    outputs = torch.softmax(outputs_logits, dim=1)
    preds = outputs.argmax(-1)
    acc = (preds == labels.view_as(preds)).float().detach().numpy().mean()
    return acc

def topk_accuracy(output, target, topk=(1,)):
    """Computes the accuracy over the k top predictions for the specified values of k"""
    with torch.no_grad():
        maxk = max(topk)
        batch_size = target.size(0)

        _, pred = output.topk(maxk, 1, True, True)
        pred = pred.t()
        correct = pred.eq(target.view(1, -1).expand_as(pred))

        res = []
        for k in topk:
            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)
            res.append(correct_k.mul_(100.0 / batch_size))
        return res

if __name__ == '__main__':
    labels = torch.rand(10)
    outputs = torch.rand((10, 4))

    acc = accuracy(labels, outputs)

'''
def split_dataset():
    xray=pd.read_csv('data/Frontal_Train.csv')
    test = xray.sample(n=500)
    train = xray.drop(test.index)

    test.to_csv("test.csv",index=False)
    train.to_csv("train.csv",index=False)
'''

'''
Freeze the model except the layers that contain str_pattern
'''
def freeze_model(model, str_pattern=["fc."]):
    #Freeze all parameters but the last one
    for name, param in model.named_parameters():
      for pattern in str_pattern:
        if pattern in name:
            continue
        else:
            param.requires_grad = False
    return(model)

'''
Freeze all layer until the specified one ('layer')
'''
def freeze_layers(model, layer):
      child_counter = 0
      for child in model.children():
          if child_counter < layer:
              #print("child ",child_counter," was frozen")
              for param in child.parameters():
                  param.requires_grad = False
          elif child_counter == layer:
            children_of_child_counter = 0
            for children_of_child in child.children():
                if children_of_child_counter < 1:
                    for param in child.parameters():                
                        param.requires_grad = False
                    #print('child ', children_of_child_counter, 'of child',child_counter,' was frozen')
                #else:
                  #print('child ', children_of_child_counter, 'of child',child_counter,' was not frozen')
                children_of_child_counter += 1
          #else:
            #print("child ",child_counter," was not frozen")
          child_counter += 1
        
      return(model)

'''
Freeze the model except the layers that contain str_pattern
'''
def load_resnet18_with_barlow_weights(barlow_state_dict_path, num_classes = 4):
    #Calling resnet model
    model = torchvision.models.resnet18(zero_init_residual=True)
    model.conv1 = nn.Conv2d(1, 64, 7, 2, 3, bias=False)

    #loading state dict and adapting it for the model (from Barlow Twins model to simple resnet model)
    if(barlow_state_dict_path != None):
        barlow_state_dict = torch.load(barlow_state_dict_path,map_location=torch.device('cpu'))
        
        state_dict = barlow_state_dict.copy()

        for k,v in barlow_state_dict.items():
            if "backbone" not in k:
              del state_dict[k]
            else:
              state_dict[k[13:]] = state_dict.pop(k)
    
        model.load_state_dict(state_dict)

    model.fc = nn.Sequential( nn.Linear(512, num_classes))
    #Adapt model and add linear projector
    return(model)

def save_checkpoint(state, basepath = os.getcwd(), filename='checkpoint.pt'):
    if not os.path.exists(basepath):
        os.makedirs(basepath)
    torch.save(state, os.path.join(basepath, filename))

class frozen_networks(nn.Module):

    def __init__(self, model_dict_resnet, batch_size, num_classes, config):
        super(frozen_networks,self).__init__()
        # Load two models
        # One model is not trained (we set the dict path as None)
        # One model is trained with previous results from trainings (Self supervised).
        
        # Freeze the model except fc layers
        self.model = load_resnet18_with_barlow_weights(model_dict_resnet,num_classes).to(config['device'])
        self.model_freeze_except_fc = freeze_model(self.model, "fc.").to(config['device'])

        # Freeze the model except layer4 + fc
        # Initialize the model every time to not keep feezing additional layers
        self.model = load_resnet18_with_barlow_weights(model_dict_resnet,num_classes).to(config['device'])
        self.model_freeze_except_l4Nfc = freeze_layers(model = self.model, layer = 7).to(config['device'])

        # Freeze the model except layer3,4 + fc
        self.model = load_resnet18_with_barlow_weights(model_dict_resnet,num_classes).to(config['device'])
        self.model_freeze_except_l34Nfc = freeze_layers(model = self.model, layer = 6).to(config['device'])

        # Freeze the model except layer2,3,4 + fc
        self.model = load_resnet18_with_barlow_weights(model_dict_resnet,num_classes).to(config['device'])
        self.model_freeze_except_l234Nfc = freeze_layers(model = self.model, layer = 5).to(config['device'])

        # Freeze the model except layer1,2,3,4 + fc
        self.model = load_resnet18_with_barlow_weights(model_dict_resnet,num_classes).to(config['device'])
        self.model_freeze_except_l1234Nfc = freeze_layers(model = self.model, layer = 4).to(config['device'])


        self.batch_size = batch_size
        self.freeze_fc = None
        self.freeze_l4Nfc = None
        self.freeze_l34Nfc = None
        self.freeze_l234Nfc = None
        self.freeze_l1234Nfc = None

    def get_dicts(self):
      return self.freeze_fc, self.freeze_l4Nfc, self.freeze_l34Nfc, self.freeze_l234Nfc, self.freeze_l1234Nfc

    def train(self, dataset_train, dataset_valid, num_epochs, criterion, config,writer):
        dataloader_train = DataLoader(dataset_train, self.batch_size)
        dataloader_valid = DataLoader(dataset_valid, self.batch_size)


        # Train on freezed model except fc layers
        optimizer = torch.optim.Adam(self.model_freeze_except_fc.parameters(), lr=config["lr_sup"], weight_decay=config["optimizer_weight_decay"])
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config["num_epochs_sup"], verbose=False)

        print(f'Starting the train of model fr except fc layers.')
        for i in range(num_epochs):
            
            loss = self.train_one_epoch(self.model_freeze_except_fc,dataloader_train,optimizer,criterion,config)
            writer.add_scalar(f"Loss/train:model fr except fc layers",loss,i)
            print("Loss/train:model fr except fc layers",loss, "Epoch =", i)
            scheduler.step()
            
            loss_valid = self.valid_one_epoch(self.model_freeze_except_fc,dataloader_valid,config)
            writer.add_scalar(f"Loss/valid:model fr except fc layers",loss_valid,i)
            print("Loss/valid:model fr except fc layers",loss_valid, "Epoch =", i)

        self.freeze_fc = self.model_freeze_except_fc.state_dict()

        # Train on freezed model except layer4 + fc layers
        optimizer = torch.optim.Adam(self.model_freeze_except_l4Nfc.parameters(), lr=config["lr_sup"], weight_decay=config["optimizer_weight_decay"])
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config["num_epochs_sup"], verbose=False)

        print("-----------------------------------------------------------------")
        print(f'Starting the train of model fr except l4 + fc layers.')
        for i in range(num_epochs):
            loss = self.train_one_epoch(self.model_freeze_except_l4Nfc,dataloader_train,optimizer,criterion,config)
            scheduler.step()
            loss_valid = self.valid_one_epoch(self.model_freeze_except_l4Nfc,dataloader_valid,config)
            writer.add_scalar(f"Loss/train:model fr except layer4 + fc layers",loss,i)
            print("Loss/train:model fr except layer4 + fc layers",loss, "Epoch = ", i)
            writer.add_scalar(f"Loss/valid:model fr except layer4 + fc layers",loss_valid,i)
            print("Loss/valid:model fr except layer4 + fc layers",loss_valid, "Epoch = ", i)

        self.freeze_l4Nfc = self.model_freeze_except_l4Nfc.state_dict()

        # Train on freezed model except layers 3 , 4 + fc layers
        optimizer = torch.optim.Adam(self.model_freeze_except_l34Nfc.parameters(), lr=config["lr_sup"], weight_decay=config["optimizer_weight_decay"])
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config["num_epochs_sup"], verbose=False)
        
        print("-----------------------------------------------------------------")
        print(f'Starting the train of model fr except layers 3,4 + fc layers.')
        for i in range(num_epochs):
            loss = self.train_one_epoch(self.model_freeze_except_l34Nfc,dataloader_train,optimizer,criterion,config)
            scheduler.step()
            loss_valid = self.valid_one_epoch(self.model_freeze_except_l34Nfc,dataloader_valid,config)
            writer.add_scalar(f"Loss/train:model fr except layers 3,4 + fc layers",loss,i)
            print("Loss/train:model fr except layers 3,4 + fc layers",loss, "Epoch = ", i)
            writer.add_scalar(f"Loss/valid:model fr except layers 3,4 + fc layers",loss_valid,i)
            print("Loss/valid:model fr except layers 3,4 + fc layers",loss_valid, "Epoch = ", i)

        self.freeze_l34Nfc = self.model_freeze_except_l34Nfc.state_dict()

        # Train on freezed model except layers 2,3,4 + fc layers
        optimizer = torch.optim.Adam(self.model_freeze_except_l234Nfc.parameters(), lr=config["lr_sup"], weight_decay=config["optimizer_weight_decay"])
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config["num_epochs_sup"], verbose=False)

        print("-----------------------------------------------------------------")
        print(f'Starting the train of model fr except layers 2,3,4 + fc layers.')
        for i in range(num_epochs):
            loss = self.train_one_epoch(self.model_freeze_except_l234Nfc,dataloader_train,optimizer,criterion,config)
            scheduler.step()
            loss_valid = self.valid_one_epoch(self.model_freeze_except_l234Nfc,dataloader_valid,config)
            writer.add_scalar(f"Loss/train:model fr except layers 2,3,4 + fc layers",loss,i)
            print("Loss/train:model fr except layers 2,3,4 + fc layers",loss, "Epoch = ", i)
            writer.add_scalar(f"Loss/valid:model fr except layers 2,3,4 + fc layers",loss_valid,i)
            print("Loss/valid:model fr except layers 2,3,4 + fc layers",loss_valid, "Epoch = ", i)
        
        self.freeze_l234Nfc = self.model_freeze_except_l234Nfc.state_dict()

        # Train on freezed model except layers 1,2,3,4 + fc layers
        optimizer = torch.optim.Adam(self.model_freeze_except_l1234Nfc.parameters(), lr=config["lr_sup"], weight_decay=config["optimizer_weight_decay"])
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config["num_epochs_sup"], verbose=False)
        
        print("-----------------------------------------------------------------")
        print(f'Starting the train of model fr except layers 1,2,3,4 + fc layers.')
        for i in range(num_epochs):
            loss = self.train_one_epoch(self.model_freeze_except_l1234Nfc,dataloader_train,optimizer,criterion,config)
            scheduler.step()
            loss_valid = self.valid_one_epoch(self.model_freeze_except_l1234Nfc,dataloader_valid,config)
            writer.add_scalar(f"Loss/train:model fr except layers 1,2,3,4 + fc layers",loss,i)
            print("Loss/train:model fr except layers 1,2,3,4 + fc layers",loss, "Epoch = ", i)
            writer.add_scalar(f"Loss/valid:model fr except layers 1,2,3,4 + fc layers",loss_valid,i)
            print("Loss/valid:model fr except layers 1,2,3,4 + fc layers",loss_valid, "Epoch = ", i) 
        
        self.freeze_l1234Nfc = self.model_freeze_except_l1234Nfc.state_dict()

           
    def train_one_epoch(self,model,dataloader_train,optimizer, criterion, config):
        running_loss = 0.0
        #logsoft = nn.LogSoftmax()
        epoch_loss = 0.0
        for i, (images,labels) in enumerate(dataloader_train):
            images, labels = images.to(config['device']), labels.to(config['device'])
            optimizer.zero_grad()

            # Add a logsoft at the end as activation function because the barlow network does not use it.
            outputs = model(images)

            loss = criterion(outputs,labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        
        return running_loss/len(dataloader_train)

    @torch.no_grad()
    def valid_one_epoch(self,model, dataloader_valid, config):
        correct_pred = 0
        total_pred = 0
        logsoft = nn.LogSoftmax()
        for i, (images,labels) in enumerate(dataloader_valid):
            images, labels = images.to(config['device']), labels.to(config['device'])
            outputs = logsoft(model(images))

            _, predicted = torch.max(outputs.data,1)

#            total_pred += labels.size(0)
#            correct_pred += (predicted.data == labels).sum().item()

            pred = outputs.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            correct_pred += pred.eq(labels.view_as(pred)).sum().item()
            print(f'Batch ended: corrected_pred={correct_pred}')
            total_pred += labels.size(0)
            print(f'Batch ended: total_pred={total_pred}')
        print("Accuracy =", correct_pred/total_pred)

        return correct_pred/total_pred


model_dict = "/content/resnet18_60epoch_CheXpert_self_supervised/resnet18_best_state_dict.pt"
#batch_size = 128
batch_size = 128
num_classes = 4
''''
ListOfCompares = [0.5]
ListOfTags=      ["50 percent of samples"]

ListOfDicts_barlow_1 = [None,None,None,None,None,None]
ListOfDicts_barlow = [None,None,None,None,None,None]
ListOfDicts_resnet_1 = [None,None,None,None,None,None]
ListOfDicts_resnet = [None,None,None,None,None,None]
'''
config = {
    'project_path': 'D:\\Documents\\GitHub\\aidl2022_final_project', 
    'model_path': 'D:\\Documents\\GitHub\\aidl2022_final_project\\runs\\final_trainings\\0.005_512__512__300', 
    'random_seed': 73, 
    'device': torch.device(type='cuda'), 
    'img_res': 224, 
    'num_classes': 4, 
    'train_frac': 0.8, 
    'test_frac': 0.1, 
    'val_frac': 0.1, 
    'transforms_prob': 0.5, 
    'h_flip': 1, 
    'batch_size_sup': 96, 
    'optimizer': 0, 
    'optimizer_weight_decay': 9.870257603212248e-06, 
    'soft_crop': 0, 
    'lr_sup': 0.0015294613483158384, 
    'num_epochs_sup': 15
    }


transform = transforms.Compose([
    
    transforms.Grayscale(),
    transforms.Resize(224),
    # you can add other transformations in this list
    transforms.ToTensor()
])

dataset = ImageFolder("/content/COVID-19_Radiography_Dataset/",transform)


val_len = int(len(dataset)*config["val_frac"])
train_len = int(len(dataset)*config["train_frac"])
test_len = int(len(dataset)-val_len - train_len)
train_dataset, val_dataset,test_dataset = torch.utils.data.random_split(dataset, [train_len, val_len,test_len])

writer = SummaryWriter(log_dir="/content/Compare_Dataset/",comment="Compare models with different freezed layers")

print(f'Total samples: {len(train_dataset)} as Training dataset')
print(f'Total samples: {len(val_dataset)} as Validation dataset')

    
comparison = frozen_networks(model_dict,batch_size,num_classes,config)
criterion = nn.CrossEntropyLoss()
comparison.train(train_dataset, val_dataset, config['num_epochs_sup'],criterion,config,writer)    
dict_fr_fc, dict_fr_l4Nfc, dict_fr_l34Nfc, dict_fr_l234Nfc, dict_fr_l1234Nfc = comparison.get_dicts()


save_checkpoint(dict_fr_fc,basepath='/content/state_dicts',filename='Barlow_15_epocs_freeze_except_fc.pt')
save_checkpoint(dict_fr_l4Nfc,basepath='/content/state_dicts',filename='Barlow_15_epocs_freeze_except_l4Nfc.pt')
save_checkpoint(dict_fr_l34Nfc,basepath='/content/state_dicts',filename='Barlow_15_epocs_freeze_except_l34Nfc.pt')
save_checkpoint(dict_fr_l234Nfc,basepath='/content/state_dicts',filename='Barlow_15_epocs_freeze_except_l234Nfc.pt')
save_checkpoint(dict_fr_l1234Nfc,basepath='/content/state_dicts',filename='Barlow_15_epocs_freeze_except_l1234Nfc.pt')



writer.flush()
writer.close()
